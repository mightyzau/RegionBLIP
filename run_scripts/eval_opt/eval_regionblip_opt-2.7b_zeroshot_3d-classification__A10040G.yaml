model:
  arch: "regionblip_opt"

  enable_pointcloud_qformer: True

  load_finetuned: False 
  load_pretrained: True 
  pretrained: 'xxx'

  load_blip2_pretrain: True 
  blip2_pretrain: '/mnt_jianchong2/pretrained/blip2_pretrained_opt2.7b__rename_for_regionblip.pth'

  bert_path: '/mnt_jianchong2/pretrained/bert-base-uncased/'

  max_txt_len: 32
  embed_dim: 256

  freeze_qformer: True        # Use BLIP2 pretrained qformer to keep image-caption ability
  qformer:
    input_dim: 1408           # Be consistent with pretrained BLIP2
    num_query_token: 32
    cross_attention_freq: 2

  freeze_vit: True
  vit:
    vit_model: "eva_clip_g"
    img_size: 364
    drop_path_rate: 0
    use_grad_checkpoint: False 
    vit_precision: "fp16"

  freeze_point_encoder: True
  pointbert:
    pretrained: "/mnt_jianchong2/pretrained/point_bert_pretrained.pt"

  opt_model: "/mnt_jianchong2/pretrained/facebook/opt-2.7b/" #"facebook/opt-2.7b"
  prompt: ""

datasets:
  mobilenet40:
    point_processor:
      train:
        name: "base_point_cloud"
    build_info:
      pointcloud:
        storage: "/mnt_jianchong2/datasets/ULIP-Objaverse_triplets/modelnet40_normal_resampled/"
        npoints: 8192

run:
  runner: runner_regionblip
  task: multimodal_3d_classification

  batch_size_eval: 16
  batch_size_train: 16
  num_workers: 4
  log_freq: 10

  max_len: 30
  min_len: 8
  num_beams: 5

  seed: 42
  output_dir: "training_dir/eval_regionblip_opt-2.7b_zeroshot_3d_classification"

  amp: True
  resume_ckpt_path: null

  evaluate: True 
  test_splits: ["test"]

  device: "cuda"
  world_size: 1
  dist_url: "env://"
  distributed: True
